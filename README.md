# featselector(特征选择器)
我们可以提取出来无数的特征，但是不一定所有的特征都是好的特征，一个坏的特征
可能会影响整个模型的好坏，有时候也有一些特征存在恨大的相关性，所有我们去除其中
的一个特征，特征提取器就是为了得到更好的有用的特征.
## 特征提取方法
这里我们抽象出来两种提取方法，一种是基于统计特征选择，另一种是基于模型的特征
选择，
### 统计相关的特征选择
叫做统计特征选择(StatFeatSelector)
1. 0-1值特征选择
    > 根据每个值的比例进行选择，如果0的比率大于0.98，我们就认为这个特征
    没有意义，可以去除。阈值可以根据业务进行设置
2. 缺失值选择
    > 如果某个特征的缺失值大于给定的阈值(例如0.8)，就去除这个特征
3. 实数值相关性特征选择
    > 计算该特征和label的相关性，回归问题根据相关性系数，分类问题计算
    Gini指数，或者增益比
4. 实数特征根据方差进行选择
    > 选择特征的方差大于某个阈值的特征
5. 根据特征间的相关方差系数选择
    > 如果特征a和b的相关系数大于 0.9, 则去除其中一个特征

### 模型特征选择
模型特征选择，又叫做包裹式特征选择，(Wrapper),我们叫做ModelFeatSelector
1. 基于树模型进行特征选择（Xgb, lgb, gbdt等gbm模型进行选择）
    > 去除重要度小于阈值的特征 


> 特征选择是机器学习中最重要的步骤，好的特征直接对应好的模型和好的结果。  
这里从目前常用的几个角度来选择好的特征。竟然是选择特征，我们就需要数据。    
这里我们给出分类和回归的两份数据集合，进行测试我们的特征选择。  
分类数据： 泰坦尼克号数据(kaggle)       
回归数据： 放假预测数据(kaggle)    





注：生活如此，问题不大。
